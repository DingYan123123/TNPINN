import os
from math import sqrt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import mean_absolute_error, mean_squared_error
from matplotlib import pyplot as plt
import pywt  # å¯¼å…¥å°æ³¢å˜æ¢åº“
from torch.utils.data import DataLoader, TensorDataset, random_split
import math
import warnings

warnings.filterwarnings('ignore')

# åŸæœ‰å‚æ•°è®¾ç½®
window_size = 10  # è®¾ç½®çª—å£å¤§å°
forecast_steps = 10  # é¢„æµ‹æ­¥æ•°
batch_size = 32  # è®¾ç½®æ‰¹æ¬¡å¤§å°
input_size = 6  # è¾“å…¥ç‰¹å¾æ•°
hidden_size = 64  # éšè—å±‚å¤§å°
output_size = forecast_steps  # è¾“å‡ºå±‚å¤§å°
epochs = 1000  # è®­ç»ƒè½®æ•°
learningrate = 0.001  # è°ƒæ•´å­¦ä¹ ç‡é€‚åº”æ–°æ¨¡å‹
weight_decay = 1e-5  # L2æ­£åˆ™åŒ–å¼ºåº¦


# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
def setup_seed(seed):
    np.random.seed(seed)  # Numpy module.
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True


# ==================== Attention-KAN-PINN æ¨¡å‹ç»„ä»¶ ====================

class BSpline(nn.Module):
    """Bæ ·æ¡åŸºå‡½æ•°å®ç°"""

    def __init__(self, grid_size=8, spline_order=3):
        super(BSpline, self).__init__()
        self.grid_size = grid_size
        self.spline_order = spline_order

        # åˆ›å»ºBæ ·æ¡èŠ‚ç‚¹
        h = 2.0 / grid_size
        grid = torch.linspace(-1 - h * spline_order, 1 + h * spline_order,
                              grid_size + 2 * spline_order + 1)
        self.register_buffer('grid', grid)

    def forward(self, x):
        """è®¡ç®—Bæ ·æ¡åŸºå‡½æ•°å€¼"""
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®èŒƒå›´å†…
        x = torch.clamp(x, -0.99, 0.99)

        # è®¡ç®—Bæ ·æ¡åŸºå‡½æ•°
        bases = []
        for i in range(self.grid_size + self.spline_order):
            basis = self.b_spline_basis(x, i, self.spline_order)
            bases.append(basis)

        return torch.stack(bases, dim=-1)

    def b_spline_basis(self, x, i, k):
        """é€’å½’è®¡ç®—Bæ ·æ¡åŸºå‡½æ•°"""
        if k == 0:
            return ((x >= self.grid[i]) & (x < self.grid[i + 1])).float()
        else:
            c1 = torch.zeros_like(x)
            c2 = torch.zeros_like(x)

            if self.grid[i + k] != self.grid[i]:
                c1 = (x - self.grid[i]) / (self.grid[i + k] - self.grid[i]) * \
                     self.b_spline_basis(x, i, k - 1)

            if self.grid[i + k + 1] != self.grid[i + 1]:
                c2 = (self.grid[i + k + 1] - x) / (self.grid[i + k + 1] - self.grid[i + 1]) * \
                     self.b_spline_basis(x, i + 1, k - 1)

            return c1 + c2


class KANLayer(nn.Module):
    """Kolmogorov-Arnoldç½‘ç»œå±‚"""

    def __init__(self, input_dim, output_dim, grid_size=8, spline_order=3):
        super(KANLayer, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.grid_size = grid_size
        self.spline_order = spline_order

        # Bæ ·æ¡åŸºå‡½æ•°
        self.bspline = BSpline(grid_size, spline_order)

        # ç³»æ•°å‚æ•°
        num_bases = grid_size + spline_order
        self.coefficients = nn.Parameter(
            torch.randn(output_dim, input_dim, num_bases) * 0.1
        )

        # å¯é€‰çš„æ®‹å·®è¿æ¥æƒé‡
        self.residual_weight = nn.Parameter(torch.randn(output_dim, input_dim) * 0.1)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size = x.shape[0]

        # è®¡ç®—Bæ ·æ¡åŸºå‡½æ•°
        basis_values = self.bspline(x)  # [batch_size, input_dim, num_basis]

        # è®¡ç®—KANè¾“å‡º
        output = torch.einsum('bin,oin->bo', basis_values, self.coefficients)

        # æ·»åŠ æ®‹å·®è¿æ¥
        residual = torch.sum(x.unsqueeze(1) * self.residual_weight.unsqueeze(0), dim=2)

        return output + residual


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)
        return output, attention_weights

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # çº¿æ€§å˜æ¢
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        # æ‹¼æ¥å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )

        # è¾“å‡ºæŠ•å½±
        output = self.W_o(attention_output)

        return output, attention_weights


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model, max_length=1000):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class PhysicsLoss(nn.Module):
    """ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°"""

    def __init__(self, physics_weight=0.1):
        super(PhysicsLoss, self).__init__()
        self.physics_weight = physics_weight

    def forward(self, predictions, inputs):
        """è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±"""
        physics_loss = torch.tensor(0.0, device=predictions.device)

        # 1. å•è°ƒæ€§çº¦æŸï¼šç”µæ± å®¹é‡åº”è¯¥å•è°ƒé€’å‡
        if predictions.shape[1] > 1:
            capacity_diff = predictions[:, 1:] - predictions[:, :-1]
            monotonicity_loss = F.relu(capacity_diff).mean()  # æƒ©ç½šæ­£çš„å·®å€¼
            physics_loss += monotonicity_loss

        # 2. è¾¹ç•Œçº¦æŸï¼šå®¹é‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        capacity_bound_loss = F.relu(predictions - 1.2).mean() + F.relu(-predictions + 0.1).mean()
        physics_loss += capacity_bound_loss

        # 3. è¡°å‡ç‡çº¦æŸï¼šè¡°å‡ç‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        if predictions.shape[1] > 2:
            decay_rate = torch.abs(predictions[:, 2:] - 2 * predictions[:, 1:-1] + predictions[:, :-2])
            decay_constraint = F.relu(decay_rate - 0.05).mean()  # é™åˆ¶è¡°å‡ç‡
            physics_loss += 0.5 * decay_constraint

        return self.physics_weight * physics_loss


class AttentionKANBlock(nn.Module):
    """æ³¨æ„åŠ›-KANèåˆå—"""

    def __init__(self, d_model, num_heads, kan_grid_size=8, dropout=0.1):
        super(AttentionKANBlock, self).__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)

        # KANå±‚
        self.kan_layer = KANLayer(d_model, d_model, kan_grid_size)

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # æ³¨æ„åŠ›å­å±‚
        attn_output, attention_weights = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))

        # KANå­å±‚
        if x.dim() == 3:  # [batch, seq, features]
            batch_size, seq_len, features = x.shape
            x_reshaped = x.view(-1, features)  # [batch*seq, features]
            kan_output = self.kan_layer(x_reshaped)
            kan_output = kan_output.view(batch_size, seq_len, -1)
        else:
            kan_output = self.kan_layer(x)

        x = self.norm2(x + self.dropout(kan_output))

        return x, attention_weights


class AttentionKANPINN(nn.Module):
    """Attention-KAN-PINNä¸»æ¨¡å‹"""

    def __init__(self,
                 input_size=6,
                 hidden_size=64,
                 output_size=10,
                 num_heads=8,
                 num_layers=3,
                 kan_grid_size=8,
                 dropout=0.1):
        super(AttentionKANPINN, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_size, hidden_size)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(hidden_size, max_length=1000)

        # Attention-KANå—å †å 
        self.attention_kan_blocks = nn.ModuleList([
            AttentionKANBlock(hidden_size, num_heads, kan_grid_size, dropout)
            for _ in range(num_layers)
        ])

        # LSTMå±‚ï¼ˆä¿æŒæ—¶åºå»ºæ¨¡èƒ½åŠ›ï¼‰
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, dropout=dropout)

        # è¾“å‡ºå±‚
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )

        # ç‰©ç†æŸå¤±
        self.physics_loss = PhysicsLoss(physics_weight=0.1)

        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡
        self.attention_weights = []

    def forward(self, x):
        # è¾“å…¥æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        x = self.input_projection(x)  # [batch_size, seq_len, hidden_size]

        # æ·»åŠ ä½ç½®ç¼–ç 
        seq_len = x.shape[1]
        x = x.transpose(0, 1)  # [seq_len, batch_size, hidden_size]
        x = self.pos_encoding(x)
        x = x.transpose(0, 1)  # [batch_size, seq_len, hidden_size]

        # é€šè¿‡Attention-KANå—
        self.attention_weights = []
        for block in self.attention_kan_blocks:
            x, attn_weights = block(x)
            self.attention_weights.append(attn_weights)

        # LSTMå±‚
        lstm_out, _ = self.lstm(x)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        final_hidden = lstm_out[:, -1, :]  # [batch_size, hidden_size]

        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(final_hidden)

        return output

    def compute_loss(self, predictions, targets, inputs):
        """è®¡ç®—æ€»æŸå¤±ï¼šæ•°æ®æŸå¤± + ç‰©ç†æŸå¤±"""
        # æ•°æ®æ‹ŸåˆæŸå¤±
        data_loss = F.mse_loss(predictions, targets)

        # ç‰©ç†çº¦æŸæŸå¤±
        physics_loss = self.physics_loss(predictions, inputs)

        # æ€»æŸå¤±
        total_loss = data_loss + physics_loss

        return total_loss, data_loss, physics_loss


# ==================== åŸæœ‰çš„è¾…åŠ©å‡½æ•° ====================

def min_max_normalization(data):
    """Min-Maxå½’ä¸€åŒ–æ–¹æ³•ï¼Œå°†æ•°æ®ç¼©æ”¾åˆ° [0, 1] èŒƒå›´å†…"""
    return (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))


def z_score_standardization(data):
    """Z-scoreæ ‡å‡†åŒ–æ–¹æ³•ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒ"""
    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)


def custom_normalization(data):
    """å½’ä¸€åŒ–å¤„ç†ï¼šå°†æ•°æ®ç¼©æ”¾åˆ° [0, 1] ä¹‹é—´"""
    capacity_col = data[:, -1]
    max_capacity = np.max(capacity_col)
    SOH = capacity_col / max_capacity
    max_SOH = np.max(SOH)
    min_SOH = np.min(SOH)
    capacity_col_normalized = 2 * (SOH - min_SOH) / (max_SOH - min_SOH) - 1
    other_cols = data[:, :-1]
    min_vals = np.min(other_cols, axis=0)
    max_vals = np.max(other_cols, axis=0)
    other_cols_normalized = 2 * (other_cols - min_vals) / (max_vals - min_vals) - 1
    return np.column_stack((other_cols_normalized, capacity_col_normalized))


def wavelet_denoising(data, wavelet='db4', level=4, threshold=0.1):
    """å°æ³¢å˜æ¢å»å™ªå‡½æ•°ï¼ˆè½¯é˜ˆå€¼æ³•ï¼‰"""
    coeffs = pywt.wavedec(data, wavelet, level=level)
    coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]
    return pywt.waverec(coeffs, wavelet)


def build_sequences(text, window_size, forecast_steps):
    """ç”Ÿæˆæ—¶åºåºåˆ—"""
    x, y = [], []
    for i in range(len(text) - window_size - forecast_steps):
        sequence = text[i:i + window_size, :]
        target = text[i + window_size:i + window_size + forecast_steps, -1]
        x.append(sequence)
        y.append(target)
    return np.array(x), np.array(y, dtype=np.float32)


def evaluation(y_test, y_predict):
    """è¯„ä¼°å‡½æ•°"""
    mae = mean_absolute_error(y_test, y_predict)
    mse = mean_squared_error(y_test, y_predict)
    rmse = sqrt(mean_squared_error(y_test, y_predict))
    return mae, mse, rmse


class EarlyStopping:
    """æ—©åœç±»"""

    def __init__(self, patience=50, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.best_score = None
        self.early_stop = False
        self.counter = 0
        self.best_model_wts = None

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.best_model_wts = model.state_dict().copy()
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_model_wts = model.state_dict().copy()
            self.counter = 0
def min_max_normalization_per_feature(data):
    # å¦‚æœè¾“å…¥æ•°æ®æ˜¯ NumPy æ•°ç»„ï¼Œå°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡
    if isinstance(data, np.ndarray):
        data = torch.tensor(data, dtype=torch.float32)

    # å¯¹æ¯ä¸ªç‰¹å¾ï¼ˆåˆ—ï¼‰è¿›è¡Œæœ€å°-æœ€å¤§å½’ä¸€åŒ–
    min_vals = torch.min(data, dim=0)[0]  # æ¯ä¸ªç‰¹å¾çš„æœ€å°å€¼ï¼Œè¿”å›æœ€å°å€¼å’Œç´¢å¼•ï¼Œè¿™é‡Œæˆ‘ä»¬å–æœ€å°å€¼
    max_vals = torch.max(data, dim=0)[0]  # æ¯ä¸ªç‰¹å¾çš„æœ€å¤§å€¼ï¼Œè¿”å›æœ€å¤§å€¼å’Œç´¢å¼•ï¼Œè¿™é‡Œæˆ‘ä»¬å–æœ€å¤§å€¼
    return (data - min_vals) / (max_vals - min_vals + 1e-6)  # å½’ä¸€åŒ–å…¬å¼ï¼Œé˜²æ­¢é™¤ä»¥0çš„é”™è¯¯

# ==================== ä¸»ç¨‹åº ====================

# æ•°æ®åŠ è½½å’Œå¤„ç†
folder_path = '../NASA Cleaned'  # æ›¿æ¢ä¸ºç”µæ± æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
battery_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]  # è·å–æ‰€æœ‰CSVæ–‡ä»¶
battery_data = {}

# å¯¹æ¯ä¸ªç”µæ± çš„æ•°æ®è¿›è¡Œå¤„ç†
for file_name in battery_files:
    file_path = os.path.join(folder_path, file_name)
    df = pd.read_csv(file_path)

    # æå–å8åˆ—ç‰¹å¾
    features = df.iloc[:, 1:].values  # è·å–å8åˆ—ç‰¹å¾
    last_column = df.iloc[:, -1].values # è·å–æœ€åä¸€åˆ—ç‰¹å¾
    # åº”ç”¨å°æ³¢å˜æ¢å»å™ªï¼ˆè½¯é˜ˆå€¼å¤„ç†ï¼‰
    # last_column = np.apply_along_axis(wavelet_denoising, 0, last_column)
    features = np.column_stack((features[:,0:5], last_column/2))
    # åº”ç”¨å°æ³¢å˜æ¢å»å™ªï¼ˆè½¯é˜ˆå€¼å¤„ç†ï¼‰
    features1 = np.apply_along_axis(wavelet_denoising, 0, features[:,0:5])  # å¯¹æ¯ä¸€åˆ—ç‰¹å¾åº”ç”¨å°æ³¢å»å™ª
    features = np.column_stack((features1, last_column))
    features = min_max_normalization_per_feature(features)
    # åˆ›å»ºæ•°æ®å’Œç›®æ ‡
    data, target = build_sequences(features, window_size, forecast_steps)

    # å°†æ¯ä¸ªç”µæ± çš„æ•°æ®ä¿å­˜åˆ°å­—å…¸ä¸­
    battery_data[file_name] = (data, target)

# äº¤å‰éªŒè¯
maes, rmses = [], []
data_losses = []
physics_losses = []

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# è®­ç»ƒè¿‡ç¨‹
for test_battery, (test_data, test_target) in battery_data.items():
    print(f"Testing on battery: {test_battery}")

    # è®­ç»ƒé›†ï¼šæ‰€æœ‰ç”µæ± æ•°æ®ï¼Œé™¤äº†å½“å‰æµ‹è¯•ç”µæ± 
    train_data = []
    train_target = []
    for battery, (data, target) in battery_data.items():
        if battery != test_battery:
            train_data.append(data)
            train_target.append(target)

    train_data = np.concatenate(train_data)
    train_target = np.concatenate(train_target)

    # è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°GPUä¸Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    train_data_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)
    train_target_tensor = torch.tensor(train_target, dtype=torch.float32).to(device)
    test_data_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)
    test_target_tensor = torch.tensor(test_target, dtype=torch.float32).to(device)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰
    train_dataset = TensorDataset(train_data_tensor, train_target_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # æ¨¡å‹å‚æ•°
    setup_seed(0)

    # ä½¿ç”¨Attention-KAN-PINNæ¨¡å‹
    model = AttentionKANPINN(
        input_size=input_size,
        hidden_size=hidden_size,
        output_size=output_size,
        num_heads=4,
        num_layers=2,
        kan_grid_size=8,
        dropout=0.1
    ).to(device)

    # å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé€‚åˆå¤æ‚æ¨¡å‹ï¼‰
    optimizer = torch.optim.Adam(model.parameters(), lr=learningrate, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    # æ—©åœå®ä¾‹åŒ–
    early_stopping = EarlyStopping(patience=50, verbose=True)

    # è®­ç»ƒæŸå¤±è®°å½•
    train_losses = []
    train_data_losses = []
    train_physics_losses = []

    print("å¼€å§‹è®­ç»ƒAttention-KAN-PINNæ¨¡å‹...")

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        epoch_data_loss = 0
        epoch_physics_loss = 0
        batch_count = 0

        for inputs, targets in train_loader:
            optimizer.zero_grad()
            inputs, targets = inputs.to(device), targets.to(device)

            # å‰å‘ä¼ æ’­
            output = model(inputs)

            # è®¡ç®—æŸå¤±
            total_loss, data_loss, physics_loss = model.compute_loss(output, targets, inputs)

            # åå‘ä¼ æ’­
            total_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            epoch_loss += total_loss.item()
            epoch_data_loss += data_loss.item()
            epoch_physics_loss += physics_loss.item()
            batch_count += 1

        if batch_count > 0:
            avg_loss = epoch_loss / batch_count
            avg_data_loss = epoch_data_loss / batch_count
            avg_physics_loss = epoch_physics_loss / batch_count

            train_losses.append(avg_loss)
            train_data_losses.append(avg_data_loss)
            train_physics_losses.append(avg_physics_loss)

            # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—æŸå¤±
            model.eval()
            with torch.no_grad():
                val_pred = model(test_data_tensor)
                val_total_loss, val_data_loss, val_physics_loss = model.compute_loss(
                    val_pred, test_target_tensor, test_data_tensor
                )

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_total_loss.item())

            # ä½¿ç”¨æ—©åœ
            early_stopping(val_total_loss.item(), model)
            if early_stopping.early_stop:
                print("Early stopping")
                model.load_state_dict(early_stopping.best_model_wts)
                break

            if epoch % 50 == 0:
                print(f'Epoch {epoch}: Total Loss={avg_loss:.6f} '
                      f'(Data={avg_data_loss:.6f}, Physics={avg_physics_loss:.6f}), '
                      f'Val Loss={val_total_loss.item():.6f}')

    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
    model.eval()
    with torch.no_grad():
        pred = model(test_data_tensor)

    # è¯„ä¼°æ¨¡å‹
    pred_np = pred.detach().squeeze().cpu().numpy()
    test_np = test_target_tensor.detach().squeeze().cpu().numpy()

    # ç¡®ä¿ç»´åº¦æ­£ç¡®
    if pred_np.ndim > 1:
        pred_np = pred_np.flatten()
    if test_np.ndim > 1:
        test_np = test_np.flatten()

    mae, mse, rmse = evaluation(test_np, pred_np)
    print(f"RMSE: {rmse * 100 :.3f}, MAE: {mae * 100 :.3f}")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    maes.append(mae)
    rmses.append(rmse)

    # ç»˜åˆ¶é¢„æµ‹ä¸çœŸå®å€¼çš„å¯¹æ¯”å›¾
    plt.figure(figsize=(8, 8))
    plt.scatter(test_np, pred_np, s=100, color='dodgerblue', alpha=0.8)
    plt.plot([min(test_np), max(test_np)], [min(test_np), max(test_np)], 'r--', label='Ideal')
    plt.xlabel("True Values")
    plt.ylabel("Predicted Values")
    # plt.title(f"Attention-KAN-PINN: {test_battery}")
    plt.legend()
    plt.grid(True)
    plt.show()

# æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ
print("\nAttention-KAN-PINN Cross-validation results:")
print(f"Average RMSE: {np.mean(rmses) * 100:.3f}")
print(f"Average MAE: {np.mean(maes) * 100:.3f}")

# ä¿å­˜è¯¦ç»†ç»“æœ
results_df = pd.DataFrame({
    'Battery': list(battery_data.keys()),
    'RMSE': [rmse * 100 for rmse in rmses],
    'MAE': [mae * 100 for mae in maes]
})

print("\nDetailed Results by Battery:")
print(results_df)

# ç»˜åˆ¶æ€»ç»“æœå¯¹æ¯”å›¾
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(range(len(rmses)), [rmse * 100 for rmse in rmses], color='skyblue', alpha=0.7)
plt.xlabel('Battery Index')
plt.ylabel('RMSE (%)')
plt.title('RMSE by Battery (Attention-KAN-PINN)')
plt.xticks(range(len(rmses)), [f'B{i + 1}' for i in range(len(rmses))])
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.bar(range(len(maes)), [mae * 100 for mae in maes], color='lightcoral', alpha=0.7)
plt.xlabel('Battery Index')
plt.ylabel('MAE (%)')
plt.title('MAE by Battery (Attention-KAN-PINN)')
plt.xticks(range(len(maes)), [f'B{i + 1}' for i in range(len(maes))])
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# æ€§èƒ½å¯¹æ¯”åˆ†æ
print("\n" + "=" * 60)
print("Attention-KAN-PINN æ¨¡å‹ç‰¹ç‚¹å’Œä¼˜åŠ¿åˆ†æ:")
print("=" * 60)
print("ğŸ¯ æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶:")
print("   â€¢ Kolmogorov-Arnoldç½‘ç»œ (KAN): å‡½æ•°çº§åˆ«çš„å¯å­¦ä¹ å‚æ•°")
print("   â€¢ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶: æ•è·åºåˆ—é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»")
print("   â€¢ ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN): é›†æˆç”µæ± è¡°å‡ç‰©ç†çº¦æŸ")
print("   â€¢ ä½ç½®ç¼–ç : ä¿æŒæ—¶åºä¿¡æ¯çš„å®Œæ•´æ€§")
print("\nğŸ”¬ ç‰©ç†çº¦æŸä¼˜åŠ¿:")
print("   â€¢ å•è°ƒæ€§çº¦æŸ: ç¡®ä¿å®¹é‡é¢„æµ‹ç¬¦åˆè¡°å‡è¶‹åŠ¿")
print("   â€¢ è¾¹ç•Œçº¦æŸ: é™åˆ¶é¢„æµ‹å€¼åœ¨ç‰©ç†åˆç†èŒƒå›´å†…")
print("   â€¢ è¡°å‡ç‡çº¦æŸ: æ§åˆ¶å®¹é‡å˜åŒ–é€Ÿç‡çš„åˆç†æ€§")
print("\nâš¡ æ¨¡å‹åˆ›æ–°ç‚¹:")
print("   â€¢ KANæ›¿ä»£ä¼ ç»ŸMLP: æ›´å¼ºçš„å‡½æ•°æ‹Ÿåˆèƒ½åŠ›")
print("   â€¢ æ³¨æ„åŠ›æœºåˆ¶: è‡ªé€‚åº”å…³æ³¨é‡è¦æ—¶é—´æ­¥")
print("   â€¢ ç‰©ç†æŸå¤±: æå‡é¢„æµ‹çš„ç‰©ç†ä¸€è‡´æ€§")
print("   â€¢ å¤šå±‚èåˆ: ç»“åˆLSTMä¿æŒæ—¶åºå»ºæ¨¡ä¼˜åŠ¿")

# æ¨¡å‹å¤æ‚åº¦åˆ†æ
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nğŸ“Š æ¨¡å‹è§„æ¨¡:")
print(f"   â€¢ æ€»å‚æ•°é‡: {total_params:,}")
print(f"   â€¢ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
print(f"   â€¢ æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.2f} MB")

# æ€§èƒ½æå‡å»ºè®®
print(f"\nğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
print("   1. è°ƒæ•´KANç½‘æ ¼å¤§å°ä»¥å¹³è¡¡ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡")
print("   2. å°è¯•ä¸åŒçš„ç‰©ç†çº¦æŸæƒé‡ç»„åˆ")
print("   3. ä½¿ç”¨æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†æå‡æ³›åŒ–èƒ½åŠ›")
print("   4. è€ƒè™‘é›†æˆå¤šä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹èåˆ")
print("   5. é’ˆå¯¹ç‰¹å®šç”µæ± ç±»å‹å¾®è°ƒè¶…å‚æ•°")

print("\n" + "=" * 60)
print("Attention-KAN-PINN æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
print("=" * 60)